# Decoupling-Prompt-and-Generation-Task-Processing-in-LLM-Inference

Large Language Model (LLM) inference confronts an inherent problem of GPU memory bottleneck, resulting in underutilization and hampering throughput. While iteration-
level scheduling has been proposed to improve LLM inference performance, it introduces elevated scheduling time and GPU idle time for advanced schedulers aimed at enhancing throughput and/or reducing response latency. Addressing the dual challenges of minimizing scheduling time and overcoming the inherent problem is imperative. In this paper, we perform an extensive experimental analysis, leading to the proposal of a Decoupled Prompt and Generation system (DPG). DPG maintains separate waiting queues for prompt and generation tasks (GT). It batches GTs with the same predicted response lengths to avoid iteration-level GT scheduling and fully utilize Key-Value (KV) cache, and adds prompt tasks to the batch to fully utilize GPU at each iteration. To further improve KV cache utilization, DPG introduces a novel KV cache use pipelining method, allowing the sharing of allocated but unused KV cache space
